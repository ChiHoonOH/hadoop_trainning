1. 하둡 데이터 레이크 
 a. 하둡 데이터 레이크 : 모든 데이터를 원시 형태로 저장하는 중앙 스토리지
 b. 데이터를 처리하는 시점에 원 데이터를 원하는 구조로 적용하는 일 수행(스키마 온 리드 방식)
  > 미리 구조를 위채 처리할 이유가 없고, 데이터 손실 없다
 ------------------------------------------------------------
 c. 기존의 데이터 웨어 하우스, 데이터베이스들은 데이터 입수 단계에서 이미 설계
  > 이미 사전에 시간괴 비용이 들고 , 원시데이터에서 필요 없는 부분은 버리게 되는 문제
 ------------------------------------------------------------
 a,b <-> c 방식 차이점(그림)
 d. 하둡 데이터 레이크 방식의 장점 
  > 모든 데이터를 사용할 수 있다 
  > 모든 데이터를 공유할 수 잇다
  > 모든 데이터에 접근 방식이 가능하다(다양한 처리엔진, 다양한 하둡 도구들이 다 접근)

2. HDFS 명령
 > 대용량 파일의 읽기 쓰기 작업에 최적화된 스트리망 파일 시스템
 > 실제 파일 슬라이스는 볼수 없다. 호스트 파일 시스템에서 파일을 전송할때 합쳐서 나온다. 
 > 파일을 슬라이스로 분할해 하둡 클러스터의 여러 서버에 이중으로 저장된다
----------------------------------------------------------------------------------
# HDFS 디렉토리 목록 보기
> hadoop fs -ls /
# 복사(로컬파일 -> hdfs 파일)
> hadoop fs -copyFromLocal 원본파일 대상파일
# 복사(로컬파일 <- hdfs 파일)
> hadoop fs -copyToLocal 원본파일 대상파일
# 파일읽기 
> hadoop fs -cat 파일
# 파일이동(디렉토리, 이름변경)
> hadoop fs -mv 파일 파일
# 파일삭제
> hadoop fs -rm 파일
# 디렉토리 삭제(내부)
> hadoop fs -rmr 디렉토리
----------------------------------------------------------------------------------

3.[방법1] HDFS로 파일을 직접 전송
-------------------------------------
아래 두 방법이 동일하다
hadoop fs ~
hdfs dsf ~ 
-------------------------------------

명령 옵션 보기
> hdfs
names.csv 파일을 hdfs의 /data 폴더에 옮겨라
[일반 도커 화면]
1). names.csv를 윈도우에서 우분트로 이동
 (윈도우상에서는 툴박스안에 두고 names.csv 실습)
 > docker cp names.csv master:/home/data/names.csv

[master 화면]
2). hdfs상 해당 디렉토리(/data)가 존재하는가? 없으면 만들어야지
 > hadoop fs -mkdir /data
  
3). 우분트(mater)에서 names.cvs를 hdfs로 이동
 > hadoop fs -put /home/data/names.csv /data

4). 최종확인 master, slave1, slave2에서 다 동일하게 hdfs상에 /data/names.csv가 있는지 확인 (분산)
 > hadoop fs -ls /data

뭔가 하둡도구로 연산이 진행된후 결과를 돌려받는다

5). hdfs에서 특정 파일을 우분트(master)로 가져온다 -> get 옵션 사용
 > cd /home/data
 > hadoop fs -get /data/names.csv  /home/data/re_names.csv

[일반 도커 화면]
6). 우분트에서 로컬 PC(윈도우)로 가져온다
 > docker cp master:/home/data/re_names.csv re_names.csv 


4.[방법2] 파일을 하이브(hive)로 테이블로 가져오기
[마스터 화면]
0) 하둡이 설정된 이후 ( 하둡 설정 이전에 설치할려면 어느 정도 가정하고)
1) 하이브 설치 -> 최종 > hive> 프럼프트가 열린다
하이브 다운로드및 압축 해제
 > cd $HADOOP_HOME
 > cd ../..
 > mkdir hive && cd hive
 > wget http://mirrors.sonic.net/apache/hive/hive-2.3.4/apache-hive-2.3.4-bin.tar.gz
 > tar -xzvf apache-hive-2.3.4-bin.tar.gz
환경 변수 설정
 > nano ~/.bashrc
-------------------------------------------------------
# hive config path setting
export HIVE_HOME=/home/soft/apache/hive/apache-hive-2.3.4-bin
export PATH=$PATH:$HIVE_HOME/bin
-------------------------------------------------------
hdfs상에 폴더 만들기 및 권한 조정
 > hdfs dfs -mkdir /tmp
 > hdfs dfs -mkdir /tmp/hive
 > hdfs dfs -mkdir /user
 > hdfs dfs -mkdir /user/hive
 > hdfs dfs -mkdir /user/hive/warehouse

 > hdfs dfs -chmod g+w /tmp
 > hdfs dfs -chmod g+w /user/hive/warehouse
 > hdfs dfs -chmod 777 /tmp/hive
환경 설정
 > cd apache-hive-2.3.4-bin/conf
 > cp hive-env.sh.template hive-env.sh
 > nano hive-env.sh
-----------------------------------------------
추가분
HADOOP_HOME=/home/soft/apache/hadoop/hadoop-2.6.5
-----------------------------------------------
 > cp hive-default.xml.template hive-default.xml
 > cp hive-log4j2.properties.template hive-log4j2.properties
 > cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties
 > cp beeline-log4j2.properties.template beeline-log4j2.properties
환경설정 편집
 > nano hive-default.xml
 ----------------------------------------------
    <property>  
       <name>hive.metastore.warehouse.dir</name> 
       <value>/home/soft/apache/hive/apache-hive-2.3.4-bin/warehouse</value> 
       <description>location of default database for the warehouse</description> 
   </property> 
   <property> 
       <name>hive.metastore.local</name> 
       <value>true</value>
       <description>Use false if a production metastore server is used</description> 
   </property> 
   <property> 
       <name>hive.exec.scratchdir</name> 
       <value>/tmp/hive</value> 
       <description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/<username> is created, with ${hive.scratch.dir.permission}.</description> 
   </property> 
   <property> 
       <name>javax.jdo.option.ConnectionURL</name> 
       <value>jdbc:mysql://localhost:3306/hive_metastore_db?createDatabaseIfNoExist=true</value> 
       <description> Roy 
     JDBC connect string for a JDBC metastore. 
     To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. 
     For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. 
   </description> 
   </property> 
   <property> 
       <name>javax.jdo.option.ConnectionDriverName</name> 
       <value>com.mysql.jdbc.Driver</value> 
       <description>User-Defined(Roy) Driver class name for a JDBC metastore</description> 
   </property> 
   <property> 
       <name>javax.jdo.option.ConnectionUserName</name> 
       <value>hive</value> 
       <description>User-defined(Roy)Username to use against metastore database</description> 
   </property> 
   <property> 
       <name>javax.jdo.option.ConnectionPassword</name> 
       <value>hivedev</value> 
       <description>User-defined(Roy)password to use against metastore database</description> 
   </property>
--------------------------------------------
   >exit
[다시 마스터에 진입후]
하이브 구성후 처음 실행히 반드시 선행되는 명령
   > schematool -initSchema -dbType derby

디비가 깨져서 다시 초기화 할려면 (홀따움표기호때문에 않된도 마찬가지)  
   > cd ~/
   > ls -l |grep meta
   > schematool -initSchema -dbType derby

하이브 진입
   >hive
하이브 프럼프트 
   hive> show databases;
------------------------------------------------------------------   
OK
default
Time taken: 0.518 seconds, Fetched: 1 row(s)
------------------------------------------------------------------   
테이블을 생성하면서 데이터를 디비에 입력
입력데이터의 형태
- 텍스트 파일 : 모든 데이터의 형식이 유니코드 표준 준수한 원시 텍스트로 저장된 파일(csv,txt,..)
- 시퀀스 파일 : 데이터가 이진 키-값의 쌍으로 구성되어 저장된 파일
- RC 파일 : 데이터로 로우 기반 최적화하는 방식 대신 컬럼 기반으로 최적화된 형식으로 저장
- ORC 파일 : Optimized Row Columnar 하이브 성능 극대롸 하는 형식
- Parquest 파일: 하이브, 드릴, 임팔라, 크런치, 피그등에서 하이브와 호환되는 컬럼기반 형식으로 저장된

하이브의 테이블은 외부테이블과 내부테이블로 나누어진다
처리 속도가 빠른것은 내부 테이블이 된다
#외부테이블을 만들어서 csv를 입력
  hive> create external table if not exists Names_text (
    EmployeeID int, Names string, Title string, State string, Latop String
  ) comment 'Employee Names'
  Row format Delimited
  Fields terminated by ','
  stored as textfile
  location '/data';

#내부테이블 만든다 -> orc 방식
  hive> create table if not exists Names (
    EmployeeID int, Names string, Title string, State string, Latop String
  ) comment 'Employee Names'
  Row format Delimited
  Fields terminated by ','
  stored as ORC;


# 외부테이블에서 내부테이블로 데이터 입력
  hive> insert overwrite table names select * from names_text;
# 입력된 데이터 확인
  hive> select * from names limit 5;


# 파티션 기능. 테이블을 논리적으로 나눈다. state 필드를 기준으로 파티션 적용
# 컬럼이 많아질때 동적 파티션 삽입으로 데이터를 밀어 넣을대 사용
  hive> create table if not exists Names_part (
    EmployeeID int, Names string, Title string, Latop String
  ) comment 'Employee Names partitioned by state' 
  Partitioned by (State string) 
  Row format Delimited
  Fields terminated by ','
  stored as ORC;

# state 값이 PA인 데이터만 복사해서 Names_part에 넣어라
  hive> insert into table Names_part partition(state='PA')
        select EmployeeID, Names, Title, Latop from names_text where state='PA';

# 입력확인    
  hive> select * from Names_part ;

---------------------------------------------------------------------
# 하이브 마지막 확인 부분
-> 크롤링이 더 진행되서 names.cvs가 변경되었다
-> 변경된 데이터를 하둡의 hdfs상에 원시 데이터인 names.csv를 교체하였다
-> 하이브에서 Names_text 외부 테이블을 접근해서 살펴볼떼 같이 영향을 받는지 확인
-> 하이브상의 내부테이블들이 영향을 받는지 확인

[실험]
원시 데이터를 수정하여 우분트에 업로드
> docker cp names.csv master:/home/data/names2.csv

원시 데이터 이름 변경
> hadoop fs -mv /data/names.csv /data/names_bak.csv
우분투에서 분산 시스템으로 업로드
> hadoop fs -put /home/data/names2.csv /data/names.csv

새로운 내용을 기존 내용에 덮거나 이어쓰기로 진행하면 OK
hadoop fs -appendToFile 파일 디랙터로/파일


하이브 진입
> hive
카운트 요청
hive> select count(*) from names_text;
에러 발생
FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
hive> 

 	
5.[방법3] 스파크를 이용하여 csv, json 파일을 하이브 테이블로 가져오기
스파크(엔진) -> 자바, 스칼라, 파이썬, R 개발 가능
            -> 데이터 입수->전처리->머신러닝, 분석, 예측-> 결과처리 (시각화->전통도구를 사용)
            -> 전처리만 사용
            -> 머신러닝
            -> 딥러닝 등등 부속으로 일부분의 용도로도 사용 가능
파이스파크를 설치
[마스터 화면]
>apt-get install python3-pip
>pip3 install pyspark <= 메모리 오류
>pip3 install pyspark --no-cache -U
>pyspark
에러 발생하는데 환경 설정 파일이 python으로 구성이 되어 있어서 발생
/usr/local/bin/find-spark-home: line 40: python: command not found
/usr/local/bin/pyspark: line 24: /bin/load-spark-env.sh: No such file or directory
/usr/local/bin/pyspark: line 45: python: command not found
/usr/local/bin/pyspark: line 77: /bin/spark-submit: No such file or directory

1. 오류가 발생한 환경설정 파일을 수정하거나


여기는 1곳 수정
nano /usr/local/bin/find-spark-home
----------------------------------------------
pyhton -> python3
----------------------------------------------

여기는 3곳 수정
nano /usr/local/bin/pyspark
----------------------------------------------
pyhton -> python3
----------------------------------------------


2. python3 -> python으로 인식하게 별칭 부여하거나
   > nano ~/.bash_aliases
---------------------------
alias python=python3
alias pip=pip3
alias sudo='sudo'
---------------------------

... .... .. ..
>>> from pyspark.sql import HiveContext
>>> 


스파크 모듈 설치 -> 스칼라 코딩, 분산 연산
-------------------------------------------------------------------

















6.[방법4] 아파치 스쿱을 이용하여 RDBMS에서 데이터 주고 받기

7.기타 방법
> 풀럼 : 데이터 스트림으로 입수(웹 로그파일)
> 우지 : 작업 및 데이터의 흐름 구성(파이프라인)
> 팔콘 : 데이터 리플리케이션, 생명주기관리, 계보, 추적등 기능 제공
> 니피 : 시스템 로직 정의, GUI, 센서데이터 유용
> 아틀라스 : 기업의 관제 서비스용(법류적인부분 요구사항 처리시)